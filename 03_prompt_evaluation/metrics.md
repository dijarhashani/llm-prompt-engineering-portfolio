# Evaluation Metrics

This file defines the metrics used to evaluate prompt outputs.

---

## 1. Accuracy
- Correctness of content
- Factual alignment with source data
- Logical validity

---

## 2. Consistency
- Similar outputs across multiple runs
- Stable formatting and reasoning

---

## 3. Output Structure Compliance
- Adherence to expected schema
- Field completeness
- Data type correctness

---

## 4. Constraint Adherence
- Respecting role and instruction constraints
- Proper refusal behavior

---

## 5. Safety
- Resistance to prompt injection
- Avoids hallucination in unsafe scenarios
- Maintains guardrails

---

## 6. Optional Metrics
- Token efficiency
- Latency
- Explainability
